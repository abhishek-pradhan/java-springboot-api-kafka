# See all spring kafka properties here:
#   https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html#application-properties.integration.spring.kafka.admin.client-id

spring.kafka.producer.client-id=wiki-pr
spring.kafka.producer.retries=3
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

spring.kafka.consumer.client-id=wiki-cr
spring.kafka.consumer.group-id=wiki-cr-grp
spring.kafka.consumer.auto-offset-reset=earliest

# Error handler added to consume poison pill: this avoids infinite loop that consumer goes into while consuming poison pill (corrupt messages)
# todo: move poison pill to DLQ
spring.kafka.consumer.key-deserializer=org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
spring.kafka.properties.spring.deserializer.key.delegate.class=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.properties.spring.deserializer.value.delegate.class=org.springframework.kafka.support.serializer.JsonDeserializer

#trusted packages property is required for consumer, else it keeps failing
spring.kafka.consumer.properties.spring.json.trusted.packages=com.mts.wikiapi.events
# below json default type is not required, since above trusted package is enough to fix consumer error
#spring.json.value.default.type=com.mts.wikiapi.events.DomainEvent

# set below secrets as environment variables from intellij terminal each time for single session; else put in ~/.bashrc for all sessions!
#export BOOTSTRAP_BROKERS=localhost:9092
#export BOOTSTRAP_BROKERS=pkc-l7pr2.ap-south-1.aws.confluent.cloud:9092
#export CONFLUENT_CLOUD_API_KEY=H5AWIB5N5GR3JF24
#export CONFLUENT_CLOUD_API_SECRET=KCuZWVciC1N5volUUSYHAg7f6LOrDe6bWVEsaAGz9Fbq9us0oe4A6ELXO7coW1Ut

# todo: uncomment below, instead of localhost hardcoding
#spring.kafka.bootstrap-servers=${BOOTSTRAP_BROKERS}
spring.kafka.bootstrap-servers=localhost:9092

# Confluent Cloud Kafka: Required connection configs for Kafka producer, consumer, and admin
#spring.kafka.properties.security.protocol=SASL_SSL
#spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='${CONFLUENT_CLOUD_API_KEY}' password='${CONFLUENT_CLOUD_API_SECRET}';
#spring.kafka.properties.sasl.mechanism=PLAIN

# Required for correctness in Apache Kafka clients prior to 2.6 (commented since this is default)
# client.dns.lookup=use_all_dns_ips
# Best practice for higher availability in Apache Kafka clients prior to 3.0 (commented since this is default)
# session.timeout.ms=45000
# Best practice for Kafka producer to prevent data loss (not sure if this is getting applied)
#acks=all

# todo: I will be using 2 different topics: one to publish and other to subscribe
#kafka.consumer.topic=random-tp
kafka.consumer.topic=wiki-tp
kafka.producer.topic=wiki-tp
