# See all spring kafka properties here:
#   https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html#application-properties.integration.spring.kafka.admin.client-id

spring.kafka.admin.client-id=wiki-adm
spring.kafka.producer.client-id=wiki-pr
spring.kafka.producer.retries=3
spring.kafka.consumer.client-id=wiki-cr
spring.kafka.consumer.group-id=wiki-cr-grp
spring.kafka.consumer.auto-offset-reset=earliest
# todo: not sure why below property is not getting applied via code
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

# Below settings are from Confluent Cloud Kafka, I have modified property names so as Spring Kafka understands it!
# I know spring.kafka.properties.sasl.jaas.config property contains credentials, but it is important to have that property here
#   so that rest of the folks get to know the actual property value. I have disabled credentials, so it won't work for anyone
# Required connection configs for Kafka producer, consumer, and admin
spring.kafka.bootstrap-servers=pkc-l7pr2.ap-south-1.aws.confluent.cloud:9092
#security.protocol=SASL_SSL
#sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='S6MNXTJYCBNSRYQZ' password='YeP279jxrmM0M9DzYiMFaKLqhEQSvGKd/qP5JzsIOfzCLEivOau65kvpXJRZ1E8o';
#sasl.mechanism=PLAIN
spring.kafka.properties.security.protocol=SASL_SSL
spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='S6MNXTJYCBNSRYQZ' password='YeP279jxrmM0M9DzYiMFaKLqhEQSvGKd/qP5JzsIOfzCLEivOau65kvpXJRZ1E8o';
spring.kafka.properties.sasl.mechanism=PLAIN

# Required for correctness in Apache Kafka clients prior to 2.6 (commented since this is default)
# client.dns.lookup=use_all_dns_ips
# Best practice for higher availability in Apache Kafka clients prior to 3.0 (commented since this is default)
# session.timeout.ms=45000
# Best practice for Kafka producer to prevent data loss (not sure if this is getting applied)
#acks=all
spring.kafka.producer.acks=all

# this is required for JSON deserialization by Kafka, else we get lots of cryptic errors! (commented this as this is not required since we are simply using app properties file to configure Spring Kafka, instead of earlier approach of code)
# spring.json.value.default.type=com.mts.wikiapi.events.DomainEvent

# todo: I will be using 2 different topics: one to publish and other to subscribe
#kafka.consumer.topic=random-tp
kafka.consumer.topic=wiki-tp1
kafka.producer.topic=wiki-tp1
